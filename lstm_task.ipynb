{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/peridot/miniconda3/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: nltk in /home/peridot/miniconda3/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/peridot/miniconda3/lib/python3.13/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/peridot/miniconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/peridot/miniconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/peridot/miniconda3/lib/python3.13/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: click in /home/peridot/miniconda3/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/peridot/miniconda3/lib/python3.13/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/peridot/miniconda3/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/peridot/miniconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/peridot/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: torch 2.11.0.dev20251228+cu128\n",
      "Uninstalling torch-2.11.0.dev20251228+cu128:\n",
      "  Successfully uninstalled torch-2.11.0.dev20251228+cu128\n",
      "Found existing installation: torchvision 0.25.0.dev20251229+cu128\n",
      "Uninstalling torchvision-0.25.0.dev20251229+cu128:\n",
      "  Successfully uninstalled torchvision-0.25.0.dev20251229+cu128\n",
      "Found existing installation: torchaudio 2.10.0.dev20251229+cu128\n",
      "Uninstalling torchaudio-2.10.0.dev20251229+cu128:\n",
      "  Successfully uninstalled torchaudio-2.10.0.dev20251229+cu128\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/torch-2.11.0.dev20251230%2Bcu128-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251230%2Bcu128-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251230%2Bcu128-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.28.9 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (2.28.9)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0+git9844da95 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torch) (3.6.0+git9844da95)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/peridot/miniconda3/lib/python3.13/site-packages (from cuda-bindings==12.9.4->torch) (1.2.2)\n",
      "Requirement already satisfied: numpy in /home/peridot/miniconda3/lib/python3.13/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/peridot/miniconda3/lib/python3.13/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/peridot/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/peridot/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/nightly/cu128/torch-2.11.0.dev20251230%2Bcu128-cp313-cp313-manylinux_2_28_x86_64.whl (916.8 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/916.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:01:46\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m198\u001b[0m, in \u001b[35m_run_module_as_main\u001b[0m\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35m_run_code\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/__main__.py\"\u001b[0m, line \u001b[35m24\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31m_main\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/cli/main.py\"\u001b[0m, line \u001b[35m80\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    return \u001b[31mcommand.main\u001b[0m\u001b[1;31m(cmd_args)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\"\u001b[0m, line \u001b[35m158\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    with \u001b[31mself.main_context\u001b[0m\u001b[1;31m()\u001b[0m:\n",
      "         \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/cli/command_context.py\"\u001b[0m, line \u001b[35m20\u001b[0m, in \u001b[35mmain_context\u001b[0m\n",
      "    with \u001b[1;31mself._main_context\u001b[0m:\n",
      "         \u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    raise exc\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
      "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m40\u001b[0m, in \u001b[35mglobal_tempdir_manager\u001b[0m\n",
      "    with \u001b[31mExitStack\u001b[0m\u001b[1;31m()\u001b[0m as stack:\n",
      "         \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    raise exc\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
      "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m167\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
      "    \u001b[31mself.cleanup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m210\u001b[0m, in \u001b[35mcleanup\u001b[0m\n",
      "    \u001b[31mrmtree\u001b[0m\u001b[1;31m(self._path, ignore_errors=False)\u001b[0m\n",
      "    \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/utils/retry.py\"\u001b[0m, line \u001b[35m37\u001b[0m, in \u001b[35mretry_wrapped\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/site-packages/pip/_internal/utils/misc.py\"\u001b[0m, line \u001b[35m128\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
      "    \u001b[31mshutil.rmtree\u001b[0m\u001b[1;31m(dir, onexc=handler)\u001b[0m  # type: ignore\n",
      "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m763\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
      "    \u001b[31m_rmtree_safe_fd\u001b[0m\u001b[1;31m(stack, onexc)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/peridot/miniconda3/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m696\u001b[0m, in \u001b[35m_rmtree_safe_fd\u001b[0m\n",
      "    \u001b[31mos.unlink\u001b[0m\u001b[1;31m(entry.name, dir_fd=topfd)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas nltk\n",
    "# Run this in a code cell\n",
    "%pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install the Nightly version (Required for RTX 5090)\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/peridot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/peridot/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/peridot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/peridot/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Imports and Setup\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4430 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 4430/4430 [00:00<00:00, 20088.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5504 passages from 4430 articles\n",
      "\n",
      "Passage DataFrame shape: (5504, 6)\n",
      "\n",
      "Sample passage:\n",
      "text            The Government of Nepal has declared a public ...\n",
      "title           NEPSE to Remain Closed on Thursday, Govt Decla...\n",
      "timestamp                               Thu, Sep 19, 2024 7:06 AM\n",
      "article_idx                                                     0\n",
      "chunk_idx                                                       0\n",
      "full_content    The Government of Nepal has declared a public ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Load and preprocess news data\n",
    "def load_news_data(json_path):\n",
    "    \"\"\"Load news articles from JSON file\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters but keep sentence structure\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:\\-\\']', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_article(content, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    Split article into overlapping chunks (passages)\n",
    "    chunk_size: target number of tokens per chunk\n",
    "    overlap: number of tokens to overlap between chunks\n",
    "    \"\"\"\n",
    "    words = content.split()\n",
    "    chunks = []\n",
    "    \n",
    "    if len(words) <= chunk_size:\n",
    "        return [content]\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "        \n",
    "        if end >= len(words):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load your data\n",
    "news_data = load_news_data('news.json')\n",
    "print(f\"Loaded {len(news_data)} articles\")\n",
    "\n",
    "# Process articles into passages\n",
    "passages = []\n",
    "passage_metadata = []\n",
    "\n",
    "for idx, article in enumerate(tqdm(news_data, desc=\"Processing articles\")):\n",
    "    title = article.get('title', '')\n",
    "    content = article.get('content', '')\n",
    "    timestamp = article.get('timestamp', '')\n",
    "    \n",
    "    # Clean text\n",
    "    clean_content = clean_text(content)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = chunk_article(clean_content)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        passages.append({\n",
    "            'text': chunk,\n",
    "            'title': title,\n",
    "            'timestamp': timestamp,\n",
    "            'article_idx': idx,\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'full_content': clean_content\n",
    "        })\n",
    "        \n",
    "        passage_metadata.append({\n",
    "            'passage_id': len(passages) - 1,\n",
    "            'article_idx': idx,\n",
    "            'title': title,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(passages)} passages from {len(news_data)} articles\")\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "passages_df = pd.DataFrame(passages)\n",
    "print(\"\\nPassage DataFrame shape:\", passages_df.shape)\n",
    "print(\"\\nSample passage:\")\n",
    "print(passages_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|██████████| 4430/4430 [00:00<00:00, 12042.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 13258 training questions\n",
      "\n",
      "Sample training questions:\n",
      "\n",
      "Q1: What is discussed about NEPSE to Remain Closed on Thursday?\n",
      "A: The Government of Nepal has declared a public holiday for today, Thursday (Ashwin 3), in celebration...\n",
      "\n",
      "Q2: What happened according to the article titled 'NEPSE to Remain Closed on Thursday, Govt Declares '?\n",
      "A: The Government of Nepal has declared a public holiday for today, Thursday (Ashwin 3), in celebration...\n",
      "\n",
      "Q3: When was the article about NEPSE published?\n",
      "A: Thu, Sep 19, 2024 7:06 AM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Generate training questions from articles\n",
    "def generate_training_questions(articles, num_questions_per_article=3):\n",
    "    \"\"\"\n",
    "    Generate training questions from article titles and content\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for idx, article in enumerate(tqdm(articles, desc=\"Generating questions\")):\n",
    "        title = article.get('title', '')\n",
    "        content = article.get('content', '')\n",
    "        \n",
    "        # Question 1: Title-based (What is discussed in this article?)\n",
    "        q1 = {\n",
    "            'question': f\"What is discussed about {title.split(':')[0].split(',')[0]}?\",\n",
    "            'article_idx': idx,\n",
    "            'answer': content[:200],  # First 200 chars as answer\n",
    "            'answer_start': 0\n",
    "        }\n",
    "        training_data.append(q1)\n",
    "        \n",
    "        # Question 2: Extract key entities (Who/What mentioned?)\n",
    "        sentences = sent_tokenize(content)\n",
    "        if len(sentences) > 0:\n",
    "            first_sent = sentences[0]\n",
    "            # Simple entity extraction (you can improve this)\n",
    "            words = first_sent.split()\n",
    "            if len(words) > 3:\n",
    "                q2 = {\n",
    "                    'question': f\"What happened according to the article titled '{title[:50]}'?\",\n",
    "                    'article_idx': idx,\n",
    "                    'answer': first_sent,\n",
    "                    'answer_start': content.find(first_sent)\n",
    "                }\n",
    "                training_data.append(q2)\n",
    "        \n",
    "        # Question 3: Date/Time based if timestamp exists\n",
    "        if article.get('timestamp'):\n",
    "            q3 = {\n",
    "                'question': f\"When was the article about {title.split()[0]} published?\",\n",
    "                'article_idx': idx,\n",
    "                'answer': article.get('timestamp', ''),\n",
    "                'answer_start': -1  # Special marker for metadata\n",
    "            }\n",
    "            training_data.append(q3)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Generate training questions\n",
    "training_questions = generate_training_questions(news_data)\n",
    "print(f\"Generated {len(training_questions)} training questions\")\n",
    "\n",
    "# Display sample questions\n",
    "print(\"\\nSample training questions:\")\n",
    "for i in range(min(3, len(training_questions))):\n",
    "    print(f\"\\nQ{i+1}: {training_questions[i]['question']}\")\n",
    "    print(f\"A: {training_questions[i]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 5504/5504 [00:02<00:00, 2255.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before trimming: 43925\n",
      "Vocabulary size after trimming: 25552\n",
      "Vocabulary saved to vocabulary.pkl\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Build vocabulary and prepare embeddings\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
    "        self.word_counts = Counter()\n",
    "        self.n_words = 4\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        self.word_counts[word] += 1\n",
    "    \n",
    "    def trim(self, min_count=2):\n",
    "        \"\"\"Remove words below min_count\"\"\"\n",
    "        keep_words = [word for word, count in self.word_counts.items() \n",
    "                     if count >= min_count]\n",
    "        \n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
    "        self.n_words = 4\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Add passages to vocabulary\n",
    "for passage in tqdm(passages, desc=\"Building vocabulary\"):\n",
    "    vocab.add_sentence(passage['text'])\n",
    "\n",
    "# Add questions to vocabulary\n",
    "for q in training_questions:\n",
    "    vocab.add_sentence(q['question'])\n",
    "    vocab.add_sentence(str(q['answer']))\n",
    "\n",
    "print(f\"Vocabulary size before trimming: {vocab.n_words}\")\n",
    "vocab.trim(min_count=2)\n",
    "print(f\"Vocabulary size after trimming: {vocab.n_words}\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open('vocabulary.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Vocabulary saved to vocabulary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([25552, 100])\n",
      "If you have GloVe embeddings, you can load them using the function provided.\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Initialize word embeddings\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "def initialize_embeddings(vocab_size, embedding_dim):\n",
    "    \"\"\"Initialize random embeddings (you can replace with GloVe/fastText)\"\"\"\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "    # PAD token should be zero\n",
    "    embeddings[0] = np.zeros(embedding_dim)\n",
    "    return torch.FloatTensor(embeddings)\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = initialize_embeddings(vocab.n_words, EMBEDDING_DIM)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# Optional: Load GloVe embeddings if you have them\n",
    "# You can download GloVe from: https://nlp.stanford.edu/projects/glove/\n",
    "# And load them here to replace random embeddings\n",
    "\n",
    "def load_glove_embeddings(glove_path, vocab, embedding_dim):\n",
    "    \"\"\"\n",
    "    Load pre-trained GloVe embeddings (optional)\n",
    "    Download from: https://nlp.stanford.edu/projects/glove/\n",
    "    \"\"\"\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (vocab.n_words, embedding_dim))\n",
    "    embeddings[0] = np.zeros(embedding_dim)  # PAD\n",
    "    \n",
    "    found = 0\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in vocab.word2idx:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[vocab.word2idx[word]] = vector\n",
    "                found += 1\n",
    "    \n",
    "    print(f\"Found {found}/{vocab.n_words} words in GloVe\")\n",
    "    return torch.FloatTensor(embeddings)\n",
    "print(\"If you have GloVe embeddings, you can load them using the function provided.\")\n",
    "# If you have GloVe, uncomment and use:\n",
    "# embedding_matrix = load_glove_embeddings('glove.6B.100d.txt', vocab, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CUDA device: NVIDIA GeForce RTX 5090\n",
      "⚠ Could not move to cuda, using CPU instead\n",
      "Retriever model created with output dim: 256\n",
      "Total parameters: 3,186,241\n",
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "# Part 6: BiLSTM Retriever Model (FIXED for CPU/CUDA compatibility)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check CUDA availability and set device properly\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        # Test if CUDA actually works\n",
    "        test_tensor = torch.zeros(1).cuda()\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"✓ Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    except:\n",
    "        print(\"⚠ CUDA available but not working, falling back to CPU\")\n",
    "        device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = True  # Fine-tune embeddings\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_dim = hidden_dim * 2\n",
    "        \n",
    "    def forward(self, input_ids, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len)\n",
    "            lengths: (batch_size,) actual lengths before padding\n",
    "        Returns:\n",
    "            encoded: (batch_size, hidden_dim * 2) fixed-size vectors\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Ensure lengths are on CPU for pack_padded_sequence\n",
    "        if lengths.is_cuda:\n",
    "            lengths_cpu = lengths.cpu()\n",
    "        else:\n",
    "            lengths_cpu = lengths\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths_cpu, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        # lstm_out: (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Attention pooling\n",
    "        attention_scores = self.attention(lstm_out)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Mask padding\n",
    "        mask = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Weighted sum\n",
    "        encoded = torch.bmm(attention_weights.unsqueeze(1), lstm_out)  # (batch_size, 1, hidden_dim * 2)\n",
    "        encoded = encoded.squeeze(1)  # (batch_size, hidden_dim * 2)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "# Initialize retriever model - DON'T move to device yet, create on CPU first\n",
    "retriever_model = BiLSTMEncoder(embedding_matrix, hidden_dim=128, num_layers=2)\n",
    "\n",
    "# Now try to move to device\n",
    "try:\n",
    "    retriever_model = retriever_model.to(device)\n",
    "    print(f\"✓ Retriever model successfully moved to {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not move to {device}, using CPU instead\")\n",
    "    device = torch.device('cpu')\n",
    "    retriever_model = retriever_model.to(device)\n",
    "\n",
    "print(f\"Retriever model created with output dim: {retriever_model.output_dim}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in retriever_model.parameters()):,}\")\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 11932 samples\n",
      "Val dataset: 1326 samples\n"
     ]
    }
   ],
   "source": [
    "# Part 7: Retriever Training Setup\n",
    "\n",
    "class RetrieverDataset(Dataset):\n",
    "    def __init__(self, questions, passages, vocab, max_len=100):\n",
    "        self.questions = questions\n",
    "        self.passages = passages\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def text_to_indices(self, text):\n",
    "        \"\"\"Convert text to word indices\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        indices = [self.vocab.word2idx.get(token, self.vocab.word2idx['<UNK>']) \n",
    "                  for token in tokens[:self.max_len]]\n",
    "        length = len(indices)\n",
    "        return indices, length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]['question']\n",
    "        article_idx = self.questions[idx]['article_idx']\n",
    "        \n",
    "        # Get positive passage (from same article)\n",
    "        positive_passages = [p for p in self.passages if p['article_idx'] == article_idx]\n",
    "        if len(positive_passages) == 0:\n",
    "            positive_passages = [self.passages[0]]  # Fallback\n",
    "        positive_passage = np.random.choice(positive_passages)\n",
    "        \n",
    "        # Get negative passage (from different article)\n",
    "        negative_passages = [p for p in self.passages if p['article_idx'] != article_idx]\n",
    "        if len(negative_passages) == 0:\n",
    "            negative_passages = [self.passages[1]]  # Fallback\n",
    "        negative_passage = np.random.choice(negative_passages)\n",
    "        \n",
    "        # Convert to indices\n",
    "        q_indices, q_len = self.text_to_indices(question)\n",
    "        pos_indices, pos_len = self.text_to_indices(positive_passage['text'])\n",
    "        neg_indices, neg_len = self.text_to_indices(negative_passage['text'])\n",
    "        \n",
    "        return {\n",
    "            'question': q_indices,\n",
    "            'q_len': q_len,\n",
    "            'positive': pos_indices,\n",
    "            'pos_len': pos_len,\n",
    "            'negative': neg_indices,\n",
    "            'neg_len': neg_len\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    questions = [item['question'] for item in batch]\n",
    "    positives = [item['positive'] for item in batch]\n",
    "    negatives = [item['negative'] for item in batch]\n",
    "    \n",
    "    q_lens = torch.LongTensor([item['q_len'] for item in batch])\n",
    "    pos_lens = torch.LongTensor([item['pos_len'] for item in batch])\n",
    "    neg_lens = torch.LongTensor([item['neg_len'] for item in batch])\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_q_len = max(len(q) for q in questions)\n",
    "    max_pos_len = max(len(p) for p in positives)\n",
    "    max_neg_len = max(len(n) for n in negatives)\n",
    "    \n",
    "    q_padded = torch.zeros(len(batch), max_q_len, dtype=torch.long)\n",
    "    pos_padded = torch.zeros(len(batch), max_pos_len, dtype=torch.long)\n",
    "    neg_padded = torch.zeros(len(batch), max_neg_len, dtype=torch.long)\n",
    "    \n",
    "    for i, (q, p, n) in enumerate(zip(questions, positives, negatives)):\n",
    "        q_padded[i, :len(q)] = torch.LongTensor(q)\n",
    "        pos_padded[i, :len(p)] = torch.LongTensor(p)\n",
    "        neg_padded[i, :len(n)] = torch.LongTensor(n)\n",
    "    \n",
    "    return {\n",
    "        'question': q_padded,\n",
    "        'q_len': q_lens,\n",
    "        'positive': pos_padded,\n",
    "        'pos_len': pos_lens,\n",
    "        'negative': neg_padded,\n",
    "        'neg_len': neg_lens\n",
    "    }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_size = int(0.9 * len(training_questions))\n",
    "train_questions = training_questions[:train_size]\n",
    "val_questions = training_questions[train_size:]\n",
    "\n",
    "train_dataset = RetrieverDataset(train_questions, passages, vocab)\n",
    "val_dataset = RetrieverDataset(val_questions, passages, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting retriever training with Manual Adam optimizer...\n",
      "Device: cpu\n",
      "Train batches: 373\n",
      "Val batches: 42\n",
      "\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 40/373 [00:15<02:08,  2.59it/s, loss=0.2736, avg=0.3169]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m train_losses, val_losses = \u001b[43mtrain_retriever_manual\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\n\u001b[32m    206\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Training completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mtrain_retriever_manual\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, lr)\u001b[39m\n\u001b[32m     98\u001b[39m neg_enc = model(neg_ids, neg_lens)\n\u001b[32m    100\u001b[39m loss = criterion(q_enc, pos_enc, neg_enc)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[32m    104\u001b[39m total_norm = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/_tensor.py:631\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:365\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# SOLUTION: Manual Adam implementation to bypass the bug\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = ((anchor - positive) ** 2).sum(dim=1).sqrt()\n",
    "        neg_dist = ((anchor - negative) ** 2).sum(dim=1).sqrt()\n",
    "        loss = (pos_dist - neg_dist + self.margin).clamp(min=0).mean()\n",
    "        return loss\n",
    "\n",
    "class ManualAdam:\n",
    "    \"\"\"Manual Adam optimizer implementation to bypass PyTorch bug\"\"\"\n",
    "    def __init__(self, parameters, lr=0.001, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.t = 0\n",
    "        \n",
    "        # Initialize moment estimates\n",
    "        self.m = [torch.zeros_like(p.data) for p in self.parameters]\n",
    "        self.v = [torch.zeros_like(p.data) for p in self.parameters]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for i, p in enumerate(self.parameters):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            \n",
    "            grad = p.grad.data\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            p.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "\n",
    "def train_retriever_manual(model, train_loader, val_loader, num_epochs=5, lr=0.001):\n",
    "    \"\"\"Training with manual Adam optimizer\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = TripletLoss(margin=0.5)\n",
    "    \n",
    "    # Use manual Adam\n",
    "    optimizer = ManualAdam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            try:\n",
    "                q_ids = batch['question'].to(device)\n",
    "                q_lens = batch['q_len'].to(device)\n",
    "                pos_ids = batch['positive'].to(device)\n",
    "                pos_lens = batch['pos_len'].to(device)\n",
    "                neg_ids = batch['negative'].to(device)\n",
    "                neg_lens = batch['neg_len'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                q_enc = model(q_ids, q_lens)\n",
    "                pos_enc = model(pos_ids, pos_lens)\n",
    "                neg_enc = model(neg_ids, neg_lens)\n",
    "                \n",
    "                loss = criterion(q_enc, pos_enc, neg_enc)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                total_norm = 0\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "                \n",
    "                clip_coef = 5.0 / (total_norm + 1e-6)\n",
    "                if clip_coef < 1:\n",
    "                    for p in model.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            p.grad.data.mul_(clip_coef)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}', 'avg': f'{total_loss/count:.4f}'})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        avg_train = total_loss / max(count, 1)\n",
    "        train_losses.append(avg_train)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val = 0\n",
    "        count_val = 0\n",
    "        \n",
    "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                try:\n",
    "                    q_ids = batch['question'].to(device)\n",
    "                    q_lens = batch['q_len'].to(device)\n",
    "                    pos_ids = batch['positive'].to(device)\n",
    "                    pos_lens = batch['pos_len'].to(device)\n",
    "                    neg_ids = batch['negative'].to(device)\n",
    "                    neg_lens = batch['neg_len'].to(device)\n",
    "                    \n",
    "                    q_enc = model(q_ids, q_lens)\n",
    "                    pos_enc = model(pos_ids, pos_lens)\n",
    "                    neg_enc = model(neg_ids, neg_lens)\n",
    "                    \n",
    "                    loss = criterion(q_enc, pos_enc, neg_enc)\n",
    "                    total_val += loss.item()\n",
    "                    count_val += 1\n",
    "                    \n",
    "                    pbar.set_postfix({'loss': f'{loss.item():.4f}', 'avg': f'{total_val/count_val:.4f}'})\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in val batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        avg_val = total_val / max(count_val, 1)\n",
    "        val_losses.append(avg_val)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train:.4f}\")\n",
    "        print(f\"  Val Loss:   {avg_val:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'train_loss': avg_train,\n",
    "                'val_loss': avg_val,\n",
    "            }, 'best_retriever_model.pth')\n",
    "            print(f\"  ✓ Best model saved!\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "        }, f'retriever_checkpoint_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Run training\n",
    "print(\"Starting retriever training with Manual Adam optimizer...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "train_losses, val_losses = train_retriever_manual(\n",
    "    retriever_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=5,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model to continue training...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.utils.serialization'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading best model to continue training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_retriever_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m retriever_model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Loaded model from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m]+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/serialization.py:1519\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch.utils.serialization'"
     ]
    }
   ],
   "source": [
    "# Continue training from the saved checkpoint\n",
    "print(\"Loading best model to continue training...\")\n",
    "\n",
    "# Load the best model\n",
    "checkpoint = torch.load('best_retriever_model.pth')\n",
    "retriever_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"✓ Loaded model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"  Previous best val loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "print(\"\\nContinuing training for 10 more epochs...\")\n",
    "train_losses_cont, val_losses_cont = train_retriever_manual(\n",
    "    retriever_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=10,  # 10 more epochs\n",
    "    lr=0.0005  # Slightly lower learning rate for fine-tuning\n",
    ")\n",
    "\n",
    "# Combine with previous losses for plotting\n",
    "train_losses_all = train_losses + train_losses_cont\n",
    "val_losses_all = val_losses + val_losses_cont\n",
    "\n",
    "print(\"\\n✓ Extended training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model to continue training...\n",
      "✓ Monkey patch applied. Try loading the model now.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'config' from 'torch.utils.serialization' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Monkey patch applied. Try loading the model now.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_retriever_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m retriever_model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Loaded model from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m]+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/serialization.py:1519\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'config' from 'torch.utils.serialization' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Continue training from the saved checkpoint\n",
    "print(\"Loading best model to continue training...\")\n",
    "\n",
    "# Load the best model\n",
    "checkpoint = torch.load('best_retriever_model.pth')\n",
    "retriever_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"✓ Loaded model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"  Previous best val loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "print(\"\\nContinuing training for 10 more epochs...\")\n",
    "train_losses_cont, val_losses_cont = train_retriever_manual(\n",
    "    retriever_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=10,  # 10 more epochs\n",
    "    lr=0.0005  # Slightly lower learning rate for fine-tuning\n",
    ")\n",
    "\n",
    "# Combine with previous losses for plotting\n",
    "train_losses_all = train_losses + train_losses_cont\n",
    "val_losses_all = val_losses + val_losses_cont\n",
    "\n",
    "print(\"\\n✓ Extended training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Loss curves\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m epochs = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_losses_all\u001b[49m)+\u001b[32m1\u001b[39m)\n\u001b[32m     10\u001b[39m plt.plot(epochs, train_losses_all, \u001b[33m'\u001b[39m\u001b[33mb-o\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mTrain Loss\u001b[39m\u001b[33m'\u001b[39m, linewidth=\u001b[32m2\u001b[39m, markersize=\u001b[32m6\u001b[39m)\n\u001b[32m     11\u001b[39m plt.plot(epochs, val_losses_all, \u001b[33m'\u001b[39m\u001b[33mr-o\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mVal Loss\u001b[39m\u001b[33m'\u001b[39m, linewidth=\u001b[32m2\u001b[39m, markersize=\u001b[32m6\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_losses_all' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the complete training history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs = range(1, len(train_losses_all)+1)\n",
    "plt.plot(epochs, train_losses_all, 'b-o', label='Train Loss', linewidth=2, markersize=6)\n",
    "plt.plot(epochs, val_losses_all, 'r-o', label='Val Loss', linewidth=2, markersize=6)\n",
    "plt.axvline(x=5, color='green', linestyle='--', alpha=0.5, label='Resumed training')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Triplet Loss', fontsize=12)\n",
    "plt.title('Complete Retriever Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement rate\n",
    "plt.subplot(1, 2, 2)\n",
    "train_improvement = [train_losses_all[0] - loss for loss in train_losses_all]\n",
    "val_improvement = [val_losses_all[0] - loss for loss in val_losses_all]\n",
    "plt.plot(epochs, train_improvement, 'b-o', label='Train Improvement', linewidth=2, markersize=6)\n",
    "plt.plot(epochs, val_improvement, 'r-o', label='Val Improvement', linewidth=2, markersize=6)\n",
    "plt.axvline(x=5, color='green', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss Reduction', fontsize=12)\n",
    "plt.title('Cumulative Improvement', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('retriever_complete_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Complete Training Statistics:\")\n",
    "print(f\"   Total Epochs:       {len(train_losses_all)}\")\n",
    "print(f\"   Initial Train Loss: {train_losses_all[0]:.4f}\")\n",
    "print(f\"   Final Train Loss:   {train_losses_all[-1]:.4f}\")\n",
    "print(f\"   Initial Val Loss:   {val_losses_all[0]:.4f}\")\n",
    "print(f\"   Final Val Loss:     {val_losses_all[-1]:.4f}\")\n",
    "print(f\"   Best Val Loss:      {min(val_losses_all):.4f} (Epoch {val_losses_all.index(min(val_losses_all))+1})\")\n",
    "print(f\"   Total Improvement:  {((train_losses_all[0] - train_losses_all[-1])/train_losses_all[0]*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, 'b-o', label='Train Loss', linewidth=2, markersize=8)\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, 'r-o', label='Val Loss', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Triplet Loss', fontsize=12)\n",
    "plt.title('Retriever Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss difference\n",
    "plt.subplot(1, 2, 2)\n",
    "overfitting = [val - train for train, val in zip(train_losses, val_losses)]\n",
    "plt.plot(range(1, len(overfitting)+1), overfitting, 'g-o', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Val Loss - Train Loss', fontsize=12)\n",
    "plt.title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('retriever_training_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Training Statistics:\")\n",
    "print(f\"   Initial Train Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"   Final Train Loss:   {train_losses[-1]:.4f}\")\n",
    "print(f\"   Initial Val Loss:   {val_losses[0]:.4f}\")\n",
    "print(f\"   Final Val Loss:     {val_losses[-1]:.4f}\")\n",
    "print(f\"   Best Val Loss:      {min(val_losses):.4f} (Epoch {val_losses.index(min(val_losses))+1})\")\n",
    "print(f\"   Train Improvement:  {((train_losses[0] - train_losses[-1])/train_losses[0]*100):.1f}%\")\n",
    "print(f\"   Val Improvement:    {((val_losses[0] - val_losses[-1])/val_losses[0]*100):.1f}%\")\n",
    "print(f\"\\n✓ Model saved as 'best_retriever_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Part 8: LSTM Reader with BiAttention for Answer Extraction\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBiAttention\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_dim):\n\u001b[32m      5\u001b[39m         \u001b[38;5;28msuper\u001b[39m(BiAttention, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 8: LSTM Reader with BiAttention for Answer Extraction\n",
    "\n",
    "class BiAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BiAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Linear(hidden_dim * 6, 1, bias=False)\n",
    "        \n",
    "    def forward(self, context, query):\n",
    "        \"\"\"\n",
    "        Bi-directional attention between context and query\n",
    "        Args:\n",
    "            context: (batch, context_len, hidden_dim)\n",
    "            query: (batch, query_len, hidden_dim)\n",
    "        Returns:\n",
    "            attended_context: (batch, context_len, hidden_dim * 4)\n",
    "        \"\"\"\n",
    "        batch_size, context_len, hidden_dim = context.shape\n",
    "        query_len = query.shape[1]\n",
    "        \n",
    "        # Expand dimensions for broadcasting\n",
    "        context_expanded = context.unsqueeze(2).expand(-1, -1, query_len, -1)\n",
    "        # (batch, context_len, query_len, hidden_dim)\n",
    "        \n",
    "        query_expanded = query.unsqueeze(1).expand(-1, context_len, -1, -1)\n",
    "        # (batch, context_len, query_len, hidden_dim)\n",
    "        \n",
    "        # Element-wise product\n",
    "        element_wise = context_expanded * query_expanded\n",
    "        # (batch, context_len, query_len, hidden_dim)\n",
    "        \n",
    "        # Concatenate [context; query; context*query]\n",
    "        combined = torch.cat([context_expanded, query_expanded, element_wise], dim=-1)\n",
    "        # (batch, context_len, query_len, hidden_dim * 3)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        similarity = self.W(combined).squeeze(-1)\n",
    "        # (batch, context_len, query_len)\n",
    "        \n",
    "        # Context-to-query attention\n",
    "        c2q_attention = F.softmax(similarity, dim=-1)\n",
    "        # (batch, context_len, query_len)\n",
    "        \n",
    "        c2q = torch.bmm(c2q_attention, query)\n",
    "        # (batch, context_len, hidden_dim)\n",
    "        \n",
    "        # Query-to-context attention\n",
    "        q2c_attention = F.softmax(similarity.max(dim=-1)[0], dim=-1)\n",
    "        # (batch, context_len)\n",
    "        \n",
    "        q2c = torch.bmm(q2c_attention.unsqueeze(1), context)\n",
    "        # (batch, 1, hidden_dim)\n",
    "        \n",
    "        q2c = q2c.expand(-1, context_len, -1)\n",
    "        # (batch, context_len, hidden_dim)\n",
    "        \n",
    "        # Combine: [context; c2q; context*c2q; context*q2c]\n",
    "        output = torch.cat([context, c2q, context * c2q, context * q2c], dim=-1)\n",
    "        # (batch, context_len, hidden_dim * 4)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTMReader(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(LSTMReader, self).__init__()\n",
    "        \n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer (shared with retriever or separate)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Question encoder\n",
    "        self.question_lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Context encoder\n",
    "        self.context_lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Bi-attention\n",
    "        self.bi_attention = BiAttention(hidden_dim * 2)\n",
    "        \n",
    "        # Modeling layer (after attention)\n",
    "        self.modeling_lstm = nn.LSTM(\n",
    "            hidden_dim * 8,  # 4 * (hidden_dim * 2) from bi-attention\n",
    "            hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layers for start and end positions\n",
    "        self.start_output = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.end_output = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, question_ids, question_lengths, context_ids, context_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            question_ids: (batch, question_len)\n",
    "            question_lengths: (batch,)\n",
    "            context_ids: (batch, context_len)\n",
    "            context_lengths: (batch,)\n",
    "        Returns:\n",
    "            start_logits: (batch, context_len)\n",
    "            end_logits: (batch, context_len)\n",
    "        \"\"\"\n",
    "        batch_size = question_ids.shape[0]\n",
    "        context_len = context_ids.shape[1]\n",
    "        \n",
    "        # Embed question and context\n",
    "        question_embedded = self.embedding(question_ids)\n",
    "        context_embedded = self.embedding(context_ids)\n",
    "        \n",
    "        # Encode question\n",
    "        packed_question = nn.utils.rnn.pack_padded_sequence(\n",
    "            question_embedded, question_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        question_output, _ = self.question_lstm(packed_question)\n",
    "        question_output, _ = nn.utils.rnn.pad_packed_sequence(question_output, batch_first=True)\n",
    "        # (batch, question_len, hidden_dim * 2)\n",
    "        \n",
    "        # Encode context\n",
    "        packed_context = nn.utils.rnn.pack_padded_sequence(\n",
    "            context_embedded, context_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        context_output, _ = self.context_lstm(packed_context)\n",
    "        context_output, _ = nn.utils.rnn.pad_packed_sequence(context_output, batch_first=True)\n",
    "        # (batch, context_len, hidden_dim * 2)\n",
    "        \n",
    "        # Bi-attention\n",
    "        attended = self.bi_attention(context_output, question_output)\n",
    "        # (batch, context_len, hidden_dim * 8)\n",
    "        \n",
    "        # Modeling layer\n",
    "        modeling_output, _ = self.modeling_lstm(attended)\n",
    "        # (batch, context_len, hidden_dim * 2)\n",
    "        \n",
    "        modeling_output = self.dropout(modeling_output)\n",
    "        \n",
    "        # Predict start and end positions\n",
    "        start_logits = self.start_output(modeling_output).squeeze(-1)\n",
    "        # (batch, context_len)\n",
    "        \n",
    "        end_logits = self.end_output(modeling_output).squeeze(-1)\n",
    "        # (batch, context_len)\n",
    "        \n",
    "        # Mask padding positions\n",
    "        context_mask = torch.arange(context_len, device=context_ids.device).unsqueeze(0) < context_lengths.unsqueeze(1)\n",
    "        start_logits = start_logits.masked_fill(~context_mask, -1e9)\n",
    "        end_logits = end_logits.masked_fill(~context_mask, -1e9)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Initialize reader model\n",
    "reader_model = LSTMReader(embedding_matrix, hidden_dim=128, num_layers=2).to(device)\n",
    "print(f\"Reader model created\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in reader_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Part 8 (continued): Reader Dataset and Preparation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mReaderDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, questions, passages, vocab, max_question_len=\u001b[32m50\u001b[39m, max_context_len=\u001b[32m400\u001b[39m):\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mself\u001b[39m.data = []\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 8 (continued): Reader Dataset and Preparation\n",
    "\n",
    "class ReaderDataset(Dataset):\n",
    "    def __init__(self, questions, passages, vocab, max_question_len=50, max_context_len=400):\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.max_question_len = max_question_len\n",
    "        self.max_context_len = max_context_len\n",
    "        \n",
    "        # Prepare training samples\n",
    "        for q in tqdm(questions, desc=\"Preparing reader dataset\"):\n",
    "            question = q['question']\n",
    "            answer = str(q['answer'])\n",
    "            article_idx = q['article_idx']\n",
    "            \n",
    "            # Find passages containing the answer\n",
    "            for passage in passages:\n",
    "                if passage['article_idx'] == article_idx:\n",
    "                    context = passage['text']\n",
    "                    \n",
    "                    # Find answer in context\n",
    "                    answer_start = context.lower().find(answer.lower())\n",
    "                    \n",
    "                    if answer_start != -1:\n",
    "                        # Convert to token positions\n",
    "                        context_tokens = word_tokenize(context.lower())\n",
    "                        answer_tokens = word_tokenize(answer.lower())\n",
    "                        \n",
    "                        # Find token-level start and end\n",
    "                        token_start, token_end = self.find_answer_span(\n",
    "                            context_tokens, answer_tokens\n",
    "                        )\n",
    "                        \n",
    "                        if token_start != -1 and token_end != -1:\n",
    "                            self.data.append({\n",
    "                                'question': question,\n",
    "                                'context': context,\n",
    "                                'answer_start': token_start,\n",
    "                                'answer_end': token_end,\n",
    "                                'context_tokens': context_tokens,\n",
    "                                'answer_text': answer\n",
    "                            })\n",
    "        \n",
    "        print(f\"Created {len(self.data)} reader training samples\")\n",
    "    \n",
    "    def find_answer_span(self, context_tokens, answer_tokens):\n",
    "        \"\"\"Find token-level start and end positions\"\"\"\n",
    "        context_len = len(context_tokens)\n",
    "        answer_len = len(answer_tokens)\n",
    "        \n",
    "        for i in range(context_len - answer_len + 1):\n",
    "            if context_tokens[i:i+answer_len] == answer_tokens:\n",
    "                return i, i + answer_len - 1\n",
    "        \n",
    "        # Fuzzy matching - find best overlap\n",
    "        best_start = -1\n",
    "        best_end = -1\n",
    "        max_overlap = 0\n",
    "        \n",
    "        for i in range(context_len - answer_len + 1):\n",
    "            overlap = sum(1 for j in range(answer_len) \n",
    "                         if i+j < context_len and context_tokens[i+j] == answer_tokens[j])\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_start = i\n",
    "                best_end = i + answer_len - 1\n",
    "        \n",
    "        if max_overlap > answer_len * 0.5:  # At least 50% match\n",
    "            return best_start, best_end\n",
    "        \n",
    "        return -1, -1\n",
    "    \n",
    "    def text_to_indices(self, text, max_len):\n",
    "        \"\"\"Convert text to indices\"\"\"\n",
    "        tokens = word_tokenize(text.lower())[:max_len]\n",
    "        indices = [self.vocab.word2idx.get(token, self.vocab.word2idx['<UNK>']) \n",
    "                  for token in tokens]\n",
    "        return indices, len(indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        q_indices, q_len = self.text_to_indices(item['question'], self.max_question_len)\n",
    "        c_indices, c_len = self.text_to_indices(item['context'], self.max_context_len)\n",
    "        \n",
    "        # Adjust answer positions if context was truncated\n",
    "        answer_start = min(item['answer_start'], c_len - 1)\n",
    "        answer_end = min(item['answer_end'], c_len - 1)\n",
    "        \n",
    "        return {\n",
    "            'question': q_indices,\n",
    "            'q_len': q_len,\n",
    "            'context': c_indices,\n",
    "            'c_len': c_len,\n",
    "            'answer_start': answer_start,\n",
    "            'answer_end': answer_end\n",
    "        }\n",
    "\n",
    "def reader_collate_fn(batch):\n",
    "    \"\"\"Collate function for reader dataloader\"\"\"\n",
    "    questions = [item['question'] for item in batch]\n",
    "    contexts = [item['context'] for item in batch]\n",
    "    \n",
    "    q_lens = torch.LongTensor([item['q_len'] for item in batch])\n",
    "    c_lens = torch.LongTensor([item['c_len'] for item in batch])\n",
    "    answer_starts = torch.LongTensor([item['answer_start'] for item in batch])\n",
    "    answer_ends = torch.LongTensor([item['answer_end'] for item in batch])\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_q_len = max(len(q) for q in questions)\n",
    "    max_c_len = max(len(c) for c in contexts)\n",
    "    \n",
    "    q_padded = torch.zeros(len(batch), max_q_len, dtype=torch.long)\n",
    "    c_padded = torch.zeros(len(batch), max_c_len, dtype=torch.long)\n",
    "    \n",
    "    for i, (q, c) in enumerate(zip(questions, contexts)):\n",
    "        q_padded[i, :len(q)] = torch.LongTensor(q)\n",
    "        c_padded[i, :len(c)] = torch.LongTensor(c)\n",
    "    \n",
    "    return {\n",
    "        'question': q_padded,\n",
    "        'q_len': q_lens,\n",
    "        'context': c_padded,\n",
    "        'c_len': c_lens,\n",
    "        'answer_start': answer_starts,\n",
    "        'answer_end': answer_ends\n",
    "    }\n",
    "\n",
    "# Create reader dataset\n",
    "reader_train_dataset = ReaderDataset(train_questions, passages, vocab)\n",
    "reader_val_dataset = ReaderDataset(val_questions, passages, vocab)\n",
    "\n",
    "reader_train_loader = DataLoader(\n",
    "    reader_train_dataset, batch_size=16, shuffle=True, collate_fn=reader_collate_fn\n",
    ")\n",
    "reader_val_loader = DataLoader(\n",
    "    reader_val_dataset, batch_size=16, shuffle=False, collate_fn=reader_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Reader train dataset: {len(reader_train_dataset)} samples\")\n",
    "print(f\"Reader val dataset: {len(reader_val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Initializing News Q&A System\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retriever_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 287\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing News Q&A System\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    284\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    286\u001b[39m qa_system = NewsQASystem(\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     retriever_model=\u001b[43mretriever_model\u001b[49m,\n\u001b[32m    288\u001b[39m     reader_model=reader_model,\n\u001b[32m    289\u001b[39m     vocab=vocab,\n\u001b[32m    290\u001b[39m     passages=passages,\n\u001b[32m    291\u001b[39m     retriever_path=\u001b[33m'\u001b[39m\u001b[33mbest_retriever_model.pth\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    292\u001b[39m     reader_path=\u001b[33m'\u001b[39m\u001b[33mbest_reader_model.pth\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    293\u001b[39m     confidence_threshold=\u001b[32m0.3\u001b[39m\n\u001b[32m    294\u001b[39m )\n\u001b[32m    296\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Q&A System ready!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 10: Complete Q&A Inference Pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class NewsQASystem:\n",
    "    def __init__(self, retriever_model, reader_model, vocab, passages, \n",
    "                 retriever_path='best_retriever_model.pth', \n",
    "                 reader_path='best_reader_model.pth',\n",
    "                 confidence_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Complete Q&A system for news articles\n",
    "        \n",
    "        Args:\n",
    "            retriever_model: Trained BiLSTM retriever\n",
    "            reader_model: Trained LSTM reader\n",
    "            vocab: Vocabulary object\n",
    "            passages: List of passage dictionaries\n",
    "            retriever_path: Path to saved retriever model\n",
    "            reader_path: Path to saved reader model\n",
    "            confidence_threshold: Minimum confidence to return an answer\n",
    "        \"\"\"\n",
    "        self.retriever = retriever_model.to(device)\n",
    "        self.reader = reader_model.to(device)\n",
    "        self.vocab = vocab\n",
    "        self.passages = passages\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Load trained models\n",
    "        print(\"Loading trained models...\")\n",
    "        retriever_checkpoint = torch.load(retriever_path, map_location=device)\n",
    "        self.retriever.load_state_dict(retriever_checkpoint['model_state_dict'])\n",
    "        self.retriever.eval()\n",
    "        \n",
    "        reader_checkpoint = torch.load(reader_path, map_location=device)\n",
    "        self.reader.load_state_dict(reader_checkpoint['model_state_dict'])\n",
    "        self.reader.eval()\n",
    "        \n",
    "        print(\"✓ Models loaded successfully!\")\n",
    "        \n",
    "        # Encode all passages for retrieval\n",
    "        print(\"Encoding passages for retrieval...\")\n",
    "        self.passage_embeddings = self._encode_all_passages()\n",
    "        print(f\"✓ Encoded {len(self.passage_embeddings)} passages\")\n",
    "    \n",
    "    def _text_to_indices(self, text, max_len=100):\n",
    "        \"\"\"Convert text to word indices\"\"\"\n",
    "        tokens = word_tokenize(text.lower())[:max_len]\n",
    "        indices = [self.vocab.word2idx.get(token, self.vocab.word2idx['<UNK>']) \n",
    "                  for token in tokens]\n",
    "        length = len(indices)\n",
    "        return indices, length, tokens\n",
    "    \n",
    "    def _encode_all_passages(self):\n",
    "        \"\"\"Encode all passages using the retriever\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for passage in tqdm(self.passages, desc=\"Encoding passages\"):\n",
    "                indices, length, _ = self._text_to_indices(passage['text'], max_len=400)\n",
    "                \n",
    "                # Convert to tensors\n",
    "                ids = torch.LongTensor([indices]).to(device)\n",
    "                lens = torch.LongTensor([length]).to(device)\n",
    "                \n",
    "                # Encode\n",
    "                embedding = self.retriever(ids, lens)\n",
    "                embeddings.append(embedding.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "    \n",
    "    def retrieve_passages(self, question, top_k=20):\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant passages for a question\n",
    "        \n",
    "        Args:\n",
    "            question: Question string\n",
    "            top_k: Number of passages to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of (passage_dict, score) tuples\n",
    "        \"\"\"\n",
    "        # Encode question\n",
    "        indices, length, _ = self._text_to_indices(question, max_len=50)\n",
    "        q_ids = torch.LongTensor([indices]).to(device)\n",
    "        q_lens = torch.LongTensor([length]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_embedding = self.retriever(q_ids, q_lens).cpu().numpy()\n",
    "        \n",
    "        # Compute similarities (cosine similarity)\n",
    "        similarities = np.dot(self.passage_embeddings, q_embedding.T).flatten()\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Return passages with scores\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'passage': self.passages[idx],\n",
    "                'score': float(similarities[idx]),\n",
    "                'passage_idx': int(idx)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_answer(self, question, passage_text):\n",
    "        \"\"\"\n",
    "        Extract answer from a passage using the reader\n",
    "        \n",
    "        Args:\n",
    "            question: Question string\n",
    "            passage_text: Passage text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, confidence, start, end positions\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        q_indices, q_len, q_tokens = self._text_to_indices(question, max_len=50)\n",
    "        c_indices, c_len, c_tokens = self._text_to_indices(passage_text, max_len=400)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        q_ids = torch.LongTensor([q_indices]).to(device)\n",
    "        q_lens = torch.LongTensor([q_len]).to(device)\n",
    "        c_ids = torch.LongTensor([c_indices]).to(device)\n",
    "        c_lens = torch.LongTensor([c_len]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_logits, end_logits = self.reader(q_ids, q_lens, c_ids, c_lens)\n",
    "            \n",
    "            # Get probabilities\n",
    "            start_probs = F.softmax(start_logits, dim=1)[0]\n",
    "            end_probs = F.softmax(end_logits, dim=1)[0]\n",
    "            \n",
    "            # Get best positions\n",
    "            start_idx = start_probs.argmax().item()\n",
    "            end_idx = end_probs.argmax().item()\n",
    "            \n",
    "            # Ensure end >= start\n",
    "            if end_idx < start_idx:\n",
    "                end_idx = start_idx\n",
    "            \n",
    "            # Get confidence (product of probabilities)\n",
    "            confidence = (start_probs[start_idx] * end_probs[end_idx]).item()\n",
    "            \n",
    "            # Extract answer text\n",
    "            if start_idx < len(c_tokens) and end_idx < len(c_tokens):\n",
    "                answer_tokens = c_tokens[start_idx:end_idx+1]\n",
    "                answer_text = ' '.join(answer_tokens)\n",
    "            else:\n",
    "                answer_text = \"\"\n",
    "            \n",
    "            return {\n",
    "                'answer': answer_text,\n",
    "                'confidence': confidence,\n",
    "                'start': start_idx,\n",
    "                'end': end_idx,\n",
    "                'start_prob': start_probs[start_idx].item(),\n",
    "                'end_prob': end_probs[end_idx].item()\n",
    "            }\n",
    "    \n",
    "    def answer_question(self, question, top_k=20, top_m=5, verbose=True):\n",
    "        \"\"\"\n",
    "        Complete pipeline: retrieve passages and extract answer\n",
    "        \n",
    "        Args:\n",
    "            question: Question string\n",
    "            top_k: Number of passages to retrieve\n",
    "            top_m: Number of passages to read\n",
    "            verbose: Print detailed output\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, confidence, citations\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        # Step 1: Retrieve passages\n",
    "        retrieved = self.retrieve_passages(question, top_k=top_k)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n📥 Retrieved {len(retrieved)} passages\")\n",
    "        \n",
    "        # Step 2: Re-rank and select top-m\n",
    "        top_passages = retrieved[:top_m]\n",
    "        \n",
    "        # Step 3: Extract answers from top passages\n",
    "        candidates = []\n",
    "        \n",
    "        for i, result in enumerate(top_passages):\n",
    "            passage = result['passage']\n",
    "            retrieval_score = result['score']\n",
    "            \n",
    "            # Extract answer\n",
    "            answer_result = self.extract_answer(question, passage['text'])\n",
    "            \n",
    "            if answer_result['answer']:\n",
    "                candidates.append({\n",
    "                    'answer': answer_result['answer'],\n",
    "                    'confidence': answer_result['confidence'],\n",
    "                    'retrieval_score': retrieval_score,\n",
    "                    'combined_score': answer_result['confidence'] * retrieval_score,\n",
    "                    'passage': passage,\n",
    "                    'title': passage['title'],\n",
    "                    'timestamp': passage['timestamp'],\n",
    "                    'article_idx': passage['article_idx']\n",
    "                })\n",
    "        \n",
    "        if not candidates:\n",
    "            if verbose:\n",
    "                print(\"\\n❌ No answer found\")\n",
    "                print(\"\\nTop sources:\")\n",
    "                for i, result in enumerate(retrieved[:3]):\n",
    "                    p = result['passage']\n",
    "                    print(f\"\\n{i+1}. {p['title']}\")\n",
    "                    print(f\"   Date: {p['timestamp']}\")\n",
    "                    print(f\"   Score: {result['score']:.3f}\")\n",
    "            \n",
    "            return {\n",
    "                'answer': 'NOT FOUND',\n",
    "                'confidence': 0.0,\n",
    "                'citations': [r['passage'] for r in retrieved[:3]]\n",
    "            }\n",
    "        \n",
    "        # Step 4: Select best answer\n",
    "        best_candidate = max(candidates, key=lambda x: x['combined_score'])\n",
    "        \n",
    "        if best_candidate['confidence'] < self.confidence_threshold:\n",
    "            if verbose:\n",
    "                print(f\"\\n⚠️  Low confidence ({best_candidate['confidence']:.3f} < {self.confidence_threshold})\")\n",
    "                print(f\"\\nBest attempt: {best_candidate['answer']}\")\n",
    "                print(\"\\nTop sources:\")\n",
    "                for i, result in enumerate(retrieved[:3]):\n",
    "                    p = result['passage']\n",
    "                    print(f\"\\n{i+1}. {p['title']}\")\n",
    "                    print(f\"   Date: {p['timestamp']}\")\n",
    "            \n",
    "            return {\n",
    "                'answer': 'NOT FOUND',\n",
    "                'confidence': best_candidate['confidence'],\n",
    "                'best_attempt': best_candidate['answer'],\n",
    "                'citations': [r['passage'] for r in retrieved[:3]]\n",
    "            }\n",
    "        \n",
    "        # Step 5: Return answer with citations\n",
    "        if verbose:\n",
    "            print(f\"\\n✅ Answer found with confidence {best_candidate['confidence']:.3f}\")\n",
    "            print(f\"\\n📝 Answer: {best_candidate['answer']}\")\n",
    "            print(f\"\\n📰 Citation:\")\n",
    "            print(f\"   Title: {best_candidate['title']}\")\n",
    "            print(f\"   Date: {best_candidate['timestamp']}\")\n",
    "            print(f\"   Retrieval Score: {best_candidate['retrieval_score']:.3f}\")\n",
    "            print(f\"   Extraction Confidence: {best_candidate['confidence']:.3f}\")\n",
    "            \n",
    "            if len(candidates) > 1:\n",
    "                print(f\"\\n📚 Alternative answers from other sources:\")\n",
    "                for i, cand in enumerate(candidates[1:4], 1):\n",
    "                    print(f\"\\n   {i}. {cand['answer']}\")\n",
    "                    print(f\"      From: {cand['title'][:60]}...\")\n",
    "                    print(f\"      Confidence: {cand['confidence']:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'answer': best_candidate['answer'],\n",
    "            'confidence': best_candidate['confidence'],\n",
    "            'title': best_candidate['title'],\n",
    "            'timestamp': best_candidate['timestamp'],\n",
    "            'article_idx': best_candidate['article_idx'],\n",
    "            'retrieval_score': best_candidate['retrieval_score'],\n",
    "            'all_candidates': candidates,\n",
    "            'top_sources': [r['passage'] for r in retrieved[:5]]\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize the Q&A System\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Initializing News Q&A System\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "qa_system = NewsQASystem(\n",
    "    retriever_model=retriever_model,\n",
    "    reader_model=reader_model,\n",
    "    vocab=vocab,\n",
    "    passages=passages,\n",
    "    retriever_path='best_retriever_model.pth',\n",
    "    reader_path='best_reader_model.pth',\n",
    "    confidence_threshold=0.3\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Q&A System ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
